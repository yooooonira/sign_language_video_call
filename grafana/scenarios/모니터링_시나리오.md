# 모니터링 테스트 시나리오

## 대상
-  drf 앱 엔드포인트 : <http://localhost:8000/metrics>
-  Redis Exporter : <http://localhost:9121/metrics>
-  Prometheus : <http://localhost:9090>
-  Alertmanager : <http://localhost:9093>


## 핵심 지표
- 인스턴스 상태 (up)
- 에러율(HTTP 5xx 비율 > 1%)
- 5xx 절대값 (RPS > 0.5)
- 지연시간 P95 > 1s


## 시나리오 1: 정상 상태 확인
**목표**: 서비스가 살아있고 메트릭이 수집되는지(up==1) 확인  
**절차**
```bash
curl -fsS http://localhost:8000/metrics | head
curl -fsS http://localhost:9121/metrics | head
```
**기대결과**
- Django/Redis 메트릭이 정상 출력됨
- Prometheus에서 up{job="django-backend"} = 1 확인
- Alertmanager 알림 없음

## 시나리오 2: 타겟 다운 알림
**목표**: app이나 Redis Exporter가 죽었을 때 알림 발생
**절차**
```bash
docker stop app   
```
**기대결과**
- Prometheus에서 up == 0 
- 알림 발생 (severity=critical)
- Alertmanager → Slack "Target down: django-backend (app:8000)" 알림 전송

## 시나리오 3: 에러율 급증 (High5xxErrorRate)
**목표**: 최근 5분간 5xx 비율 > 1% 일 때 알림
**절차**
```bash
for i in {1..200}; do curl -s http://localhost:8000/_debug/fail?rate=0.2 >/dev/null; done
```

**기대결과**
- Prometheus에서 High5xxErrorRate 조건 만족
- Alertmanager → Slack "5xx 오류율 > 1%" 알림 전송

## 시나리오 4: 5xx 절대값 초과 (High5xxAbsoluteRPS)
**목표**: 트래픽이 초당 0.5건 이상 5xx 발생 시 알림
**절차**
```bash
ab -n 200 -c 10 http://localhost:8000/_debug/fail?rate=1
```
**기대결과**
- Prometheus에서 High5xxAbsoluteRPS 조건 만족
- Alertmanager → Slack "5xx 초당 실패 건수 > 0.5" 알림 전송

## 시나리오 5: 지연시간 P95 상승 (HighLatencyP95)
**목표**: 최근 10분간 P95 응답시간 > 1초
**절차**
```bash
ab -n 100 -c 5 http://localhost:8000/api/items?sleep=1.5
```
**기대결과**
- Prometheus에서 HighLatencyP95 조건 충족
- Alertmanager → Slack "P95 응답시간 1s 초과" 알림 전송